{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cheat sheet contains oparations on natural language processing and string manipulations in Python.\n",
    "\n",
    "Sources:\n",
    "- https://github.com/moondra2017/Python-Regular-Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic built-in text manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Despite the CONSTANT negative press, covfefe\"\n",
    "print(\"Number of characters\",len(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split individual words by different separation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Despite the CONSTANT negative press, covfefe\"\n",
    "print(str1.split())\n",
    "print(str1.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stripping whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = ' hello  apple '\n",
    "\n",
    "print(str1.strip()) # leading and trailing whitespaces\n",
    "print(str1.replace(\" \", \"\")) # all whitespaces\n",
    "\n",
    "# Needs RE!\n",
    "print(re.sub( '\\s+', ' ', str1).strip()) # replace multiple whitespaces with one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force between lower and upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Despite the CONSTANT negative press, covfefe\"\n",
    "print([w.lower() for w in str1.split()])\n",
    "print([w.upper() for w in str1.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find specific words using list comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Despite the CONSTANT negative press, covfefe\"\n",
    "print([w for w in str1.split() if len(w) > 7])\n",
    "print([w for w in str1.split() if w.istitle()])\n",
    "print([w for w in str1.split() if w.islower()])\n",
    "print([w for w in str1.split() if w.isupper()])\n",
    "print([w for w in str1.split() if w.endswith('fe')])\n",
    "print([w for w in str1.split() if w.startswith('c')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Despite the CONSTANT negative press, covfefe covfefe covfefe\"\n",
    "print(set(str1.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract numeric types from string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "mystring = \"61-63sds.0600\"\n",
    "print(int(\"\".join(itertools.takewhile(str.isdigit, mystring))))\n",
    "\n",
    "mylist = [\"\".join(x) for _, x in itertools.groupby(mystring, key=str.isdigit)]\n",
    "mylist = [s for s in mylist if s.isdigit()]\n",
    "print(mylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regular expressions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick quide to different characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick guide to re <b>meta characters</b>\n",
    "\n",
    "<b>Identifiers</b>\n",
    "- \\s is any whitespace character [ \\t\\n\\r\\f\\v]\n",
    "- \\S is any non-whitespace character [^ \\t\\n\\r\\f\\v]\n",
    "- \\b whitespace around words\n",
    "- \\d <=> [0-9]\n",
    "- \\D <=> [^0-9]\n",
    "- \\w is any alphanumeric character <=> [a-zA-Z0-9_]\n",
    "- \\W is any non-alphanumeric character <=> [^a-zA-Z0-9_]\n",
    "- . any character, except for a newline\n",
    "- \\\\. a period\n",
    "\n",
    "<b>Modifiers</b>\n",
    "- \\{n\\} exactly n repetitions where $n \\geq 0$\n",
    "- \\{n , \\} at least $n$ repetitions\n",
    "- \\{ , n\\} at most $n$ repetitions\n",
    "- \\{m , n\\} at least $m$ and at most $n$ repetitions\n",
    "- \\+ match one or more occurences\n",
    "- \\* match zero or more occurences\n",
    "- ? match zero or one occurences\n",
    "- \\$ match the end of a string; match needs to be at the end of a string\n",
    "- ^ match the beginning of a string; match needs to be at the beginning of a string\n",
    "- | either or; a|b matches either a or b\n",
    "- [] Used to indicate a set of characters\n",
    "\n",
    "\n",
    "<b>White space characters</b>\n",
    "- \\n new line\n",
    "- \\s space\n",
    "- \\t tab\n",
    "- \\e escape\n",
    "- \\f form feed (?)\n",
    "- \\r return\n",
    "\n",
    "<b>Word boundary for non-alphanumeric cahracters</b>\n",
    "- \\b word boundary; looks on one side nonalpha numeric caracter and on another alpha numeric character\n",
    "- \\B opposite of word boundary; looks both sides for alpha numeric character OR both sides for nonalpha numeric character\n",
    "\n",
    "\n",
    "<b>Groups</b>\n",
    "- () Defines a group. Only grouped stuff gets pulled out but whole re will be matched\n",
    "- (?:) A non-capturing version of regular parentheses. Matches whatever regular expression is inside the parentheses, but the substring matched by the group cannot be retrieved after performing a match or referenced later in the pattern.\n",
    "- (?P) Naming groups\n",
    "\n",
    "<b>Examples</b>\n",
    "\n",
    "- [a-z] matches lowercase set of charactes a to z\n",
    "- [A-Z] matches uppercase set of charactes A to Z\n",
    "- [0-9] matches numbers 0 to 9\n",
    "- [^b-n] matches lowercase set of charactes except letters b to n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alphanumeric characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These are alphanumeric characters\n",
    "- A-Z\n",
    "- a-z\n",
    "- 0-9\n",
    "\n",
    "These are not:\n",
    "- . (dot\n",
    "-  (space)\n",
    "- @\n",
    "- any newline character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- re.MULTILINE (re.M): Looks for instances separately at each new line. Used in conjunction with meta characters '^' or '$'. For example, in conjunction with '^' what is getting matched is beginning of each new line. Works only with re.search()!\n",
    "- re.IGNORECASE: Ignores cases\n",
    "- re.DOTALL (re.S): Used in conjunction with meta character '.' (dot) to actually include newlines as well.\n",
    "- re.ASCII\n",
    "- re.DEBUG\n",
    "- re.LOCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some flag examples\n",
    "string = '''U.S. stock-index futures pointed\n",
    "to a solidly higher open on\n",
    "Monday, indicating that major\n",
    "benchmarks were poised to rebound\n",
    "from last week’s sharp decline,\n",
    "which represented their biggest weekly drops in months.\n",
    "That weakness was driven in part by\n",
    "fears over North Korea, where tensions\n",
    "with the U.S. have been escalating.\n",
    "North Korea. That issue overshadowed the state of\n",
    "the equity market, where earnings\n",
    "have been strong at a time of high\n",
    "employment and low inflation,\n",
    "as well as valuations that\n",
    "appear elevated by many metrics, north korea North Korea.'''\n",
    "\n",
    "pattern1a = re.compile('^North Korea\\.?', flags = re.MULTILINE)\n",
    "pattern1b = re.compile('^North Korea\\.?')\n",
    "\n",
    "pattern2a = re.compile('north korea', flags = re.IGNORECASE)\n",
    "pattern2b = re.compile('north korea')\n",
    "\n",
    "pattern3a = re.compile('.*', flags = re.S)\n",
    "pattern3b = re.compile('.*')\n",
    "\n",
    "print(re.search(pattern1a, string))\n",
    "print(re.search(pattern1b, string))\n",
    "print('------')\n",
    "print(re.findall(pattern2a, string))\n",
    "print(re.findall(pattern2b, string))\n",
    "print('------')\n",
    "print(re.match(pattern3a, string).group())\n",
    "print('--')\n",
    "print(re.match(pattern3b, string).group())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using re.findall(), only grouped stuff will get outputted! This is not tha same with re.search() or re.match() as they always output the entire match by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'John has 6 cats but I think my friend Susan has 3 dogs and Mike has 8 fishes'\n",
    "\n",
    "myre1 = re.compile('[A-Za-z]+ \\w+ \\d+ \\w+')\n",
    "myre2 = re.compile('([A-Za-z]+) \\w+ \\d+ \\w+')\n",
    "myre3 = re.compile('([A-Za-z]+) \\w+ \\d+ (\\w+)')\n",
    "myre4 = re.compile('(([A-Za-z]+) \\w+ \\d+ (\\w+))')\n",
    "    \n",
    "print(re.findall(myre1, string))\n",
    "print(re.findall(myre2, string))\n",
    "print(re.findall(myre3, string))\n",
    "print(re.findall(myre4, string))\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(re.search(myre1, string).group())\n",
    "print(re.search(myre2, string).group())\n",
    "print(re.search(myre3, string).group())\n",
    "print(re.search(myre4, string).group())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although re.search() and re.match() output the entire match as default '0' group, they can (and will) distinguish (only) between individual groups that are captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'John has 6 cats but I think my friend Susan has 3 dogs and Mike has 8 fishes'\n",
    "myre1 = re.compile('([A-Za-z]+) (\\w+) \\d+ (\\w+)')\n",
    "myre2 = re.compile('([A-Za-z]+) (?:\\w+) \\d+ (\\w+)')\n",
    "\n",
    "print(re.search(myre1, string).group())\n",
    "print(re.search(myre1, string).group(1))\n",
    "print(re.search(myre1, string).group(2))\n",
    "print(re.search(myre1, string).group(3))\n",
    "print(re.search(myre1, string).groups())\n",
    "print(re.search(myre1, string).group(2,1))\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(re.search(myre2, string).group())\n",
    "print(re.search(myre2, string).group(1))\n",
    "print(re.search(myre2, string).group(2))\n",
    "#print(re.search(myre2, string).group(3)) # there is only 2 captured groups!\n",
    "print(re.search(myre2, string).groups())\n",
    "print(re.search(myre2, string).group(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re.match() and re.serach() have all kinds of methods beside .group() and .groups():\n",
    "- .span()\n",
    "- .start()\n",
    "- .end()\n",
    "\n",
    "re.findall() does not have any of these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'John has 6 cats but I think my friend Susan has 3 dogs and Mike has 8 fishes'\n",
    "myre1 = re.compile('([A-Za-z]+) (\\w+) \\d+ (\\w+)')\n",
    "\n",
    "print(re.search(myre1, string))\n",
    "print(re.search(myre1, string).span())\n",
    "print(re.search(myre1, string).span(2))\n",
    "print(re.search(myre1, string).start())\n",
    "print(re.search(myre1, string).start(3))\n",
    "print(re.search(myre1, string).end())\n",
    "print(re.search(myre1, string).end(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backreferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use syntax \\1 to refer to the 1st group within regular expression\n",
    "# So essentially we want to find 1st group twice\n",
    "\n",
    "string = 'Merry Merry Christmas'\n",
    "\n",
    "myre1 = re.compile(r'(\\w+) \\1')\n",
    "myre2 = re.compile('(\\w+) \\1') # doe snot work without raw string!\n",
    "\n",
    "\n",
    "print(re.search(myre1, string))\n",
    "print(re.search(myre2, string))\n",
    "\n",
    "# re.findall() only outputs the word once!\n",
    "print(re.findall(myre1, string))\n",
    "print(re.findall(myre2, string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capturing vs. non-capturing groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-capturing groups can be used when group structure wants to be used but group does not necessarily want to be included in the match. For example, in cases where a group might be present (1 or more occurence) but not necessarily (0 occurences); if zero occurences, then we donät want to output this as it will be jsut empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "    Feb-25-2001; Feb 25, 2001; February 25, 2001; Feb. 25, 2001; Feb 25 2009;\n",
    "'''\n",
    "# With this findall prints 'zeroeth group' only\n",
    "pattern1 = re.compile(r'(?:\\d{2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* (?:\\d{1,2}, )?\\d{4}')\n",
    "\n",
    "# With this groups 1-3 get outputted with findall. 3rd group is only the day and not the year since year is not in group\n",
    "pattern2 = re.compile(r'(\\d{2} )?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* (\\d{1,2}, )?\\d{4}')\n",
    "\n",
    "#Notice that re.search matches the same for both patterns!\n",
    "print(re.search(pattern1, string).group(0))\n",
    "print(re.findall(pattern1, string))\n",
    "print('-'*15)\n",
    "print(re.search(pattern2, string).group(0))\n",
    "print(re.findall(pattern2, string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below example shows that non-captured groups get matched in the \"group 0\" but not as actual groups.\n",
    "\n",
    "Source: https://stackoverflow.com/questions/3512471/what-is-a-non-capturing-group-what-does-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = 'https://stackoverflow.com/'\n",
    "\n",
    "# protocol part as capturing\n",
    "exp1 = re.compile(r'(https?|ftp)://([^/\\r\\n]+)(/[^\\r\\n]*)?')\n",
    "\n",
    "# protocol part as non-capturing\n",
    "exp2 = re.compile(r'(?:https?|ftp)://([^/\\r\\n]+)(/[^\\r\\n]*)?')\n",
    "\n",
    "print(re.search(exp1, string1).group(0))\n",
    "print(re.search(exp1, string1).group(1))\n",
    "print(re.search(exp1, string1).group(2))\n",
    "print(re.search(exp1, string1).group(3))\n",
    "print('-'*15)\n",
    "print(re.search(exp2, string1).group(0))\n",
    "print(re.search(exp2, string1).group(1))\n",
    "print(re.search(exp2, string1).group(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = 'https://stackoverflow.com/'\n",
    "\n",
    "# protocol part as capturing\n",
    "exp1 = re.compile(r'(https?|ftp)://([^/\\r\\n]+)(/[^\\r\\n]*)?')\n",
    "\n",
    "# protocol part as non-capturing\n",
    "exp2 = re.compile(r'(?:https?|ftp)://([^/\\r\\n]+)(/[^\\r\\n]*)?')\n",
    "\n",
    "print(re.findall(exp1, string1))\n",
    "print(re.findall(exp2, string1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitutes parts in a string. Works as re.findall() in that it searches all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string =\"\"\"U.S. stock-index futures pointed\n",
    "to a solidly higher open on Monday, indicating\n",
    "that major benchmarks were poised to USA\n",
    "rebound from last week’s sharp decline, which\n",
    "represented to us their biggest weekly drops in months.\"\"\"\n",
    "\n",
    "print(re.sub('U.S.|US|USA', 'United States ', string ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Dan has 3 snails. Mike has 4 cats. Alisa has 9 monkeys.'\n",
    "square = lambda x: x**2\n",
    "\n",
    "print(re.sub('(\\d+)', '1', string))\n",
    "print(re.sub('(\\d+)', lambda x: str(x.group(0)), string))\n",
    "print(re.sub('(\\d+)', lambda x: str(3 + int(x.group(0))), string))\n",
    "print(re.sub('(\\d+)', lambda x: str(square(int(x.group(0)))), string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word boundaru checks both sides of a word for nonalpha numeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"cat catherine catholic wildcat copycat uncatchable\"\n",
    "string2 = \".cat catherine catholic wildcat copycat uncatchable\"\n",
    "\n",
    "pattern1 = re.compile(r'\\bcat\\b')\n",
    "\n",
    "print(re.findall(pattern1, string1))\n",
    "# Notice that dot is nonalpha numeric!\n",
    "print(re.findall(pattern1, string2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookarounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few needed terms which are not complements to each other, more like synonyms!\n",
    "\n",
    "<b>'Capturing'</b>: Used only in relation to groups; we have capturing groups and non-capturing groups. Captured groups get stored as actual groups. However, non-capturing groups are consumed (unlike lookarounds which are NOT consumed) and thus matched in the \"group zero\". They are not included as a separate group though. See section Capturing vs. non-capturing groups.<br>\n",
    "<b>'Consuming'</b>: Moving through a string that everything that matches gets consumed, and the cursor moves to end of the match. Consumed parts will get outputted\n",
    "\n",
    "- A group, either capturing or non-capturing, is consumed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookbacks allow us to confirm that some sort of subpattern is ahead or behind main pattern\n",
    "\n",
    "- ?= Positive lookahead\n",
    "- ?! Negative lookahead\n",
    "- ?<= Positive lookback\n",
    "- ?<! Negative lookback\n",
    "\n",
    "When you use non-capturing group it is consuming the characters. Due to this it cannot deal with overlapping groups. Lookarounds do not consume! For example, in positive lookahead cursor moves and looks for match but comes back if match was found.\n",
    "\n",
    "I think lookarounds always need a group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example positive lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string ='''ABC1    1.1.1.1    20151118    active\n",
    "           ABC2    2.2.2.2    20151118    inactive\n",
    "           ABC3    x.x.x.x    xxxxxxxx    active'''\n",
    "\n",
    "# Regular expression with capturing group of second column and last column\n",
    "# with positive lookahead for word 'active'\n",
    "# Group with positive lookahead is not being captured since it is a 'zero width assertion'\n",
    "myre1 = 'ABC\\w\\s+(\\S+)\\s+\\S+\\s+(?=active)'\n",
    "pattern1 =re.compile(myre1)\n",
    "\n",
    "# Same using non-capturing group syntax. It is consuming the characters, but not capturing\n",
    "# it to a group. This means it will be included in the match BUT it won't be outputted with findall\n",
    "myre2 = 'ABC\\w\\s+(\\S+)\\s+\\S+\\s+(?:active)'\n",
    "pattern2 =re.compile(myre2)\n",
    "\n",
    "# In contrast to myre2, here the last group is indeed capturing\n",
    "# and it gets outputted with findall\n",
    "myre3 = 'ABC\\w\\s+(\\S+)\\s+\\S+\\s+(active)'\n",
    "pattern3 =re.compile(myre3)\n",
    "\n",
    "\n",
    "print(re.findall(pattern1, string))\n",
    "print(re.search(pattern1, string))\n",
    "print(re.search(pattern1, string).group())\n",
    "\n",
    "print('----')\n",
    "print(re.findall(pattern2, string))\n",
    "print(re.search(pattern2, string))\n",
    "print(re.search(pattern2, string).group())\n",
    "\n",
    "print('----')\n",
    "\n",
    "print(re.findall(pattern3, string))\n",
    "print(re.search(pattern3, string))\n",
    "print(re.search(pattern3, string).group())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example negative lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "Remaining party applicants:\n",
    "\n",
    "Occupation: Party Planner\n",
    "Occupation: Baking\n",
    "Occupation: Cook\n",
    "Occupation: Economist\n",
    "Occupation: Publicist\n",
    "Occupation: Baker\n",
    "Occupation: baker\n",
    "Occupation: pierrot'''\n",
    "\n",
    "# Find those with occupation not dealing with cooking or baking\n",
    "# Notice that we need .+ to capture the matched occupation names\n",
    "pattern = re.compile('Occupation: (?!Baker|Baking|Cook).+', flags = re.IGNORECASE)\n",
    "\n",
    "print(re.findall(pattern,string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example negative lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to extract names from persons with baker not being their occupation\n",
    "string = '''\n",
    "Remaining party applicants:\n",
    "\n",
    "Planner: Joe Doe\n",
    "Guest: Maria Jackson\n",
    "Cook: Sarah Jones\n",
    "Economist: Josefina Vilar\n",
    "Publicist: Mark Garm\n",
    "Baker: Santa Claus\n",
    "Party Planner: Misty Mountains\n",
    "baker: Seema Patel\n",
    "pierrot: Bill Smith'''\n",
    "\n",
    "# This is the working version\n",
    "pattern1 = re.compile(r'(?<!Baker: )\\b\\w+\\s\\w+$', flags = re.IGNORECASE|re.M)\n",
    "\n",
    "#Problem version: needs to be a raw string otherwise no match\n",
    "pattern2 = re.compile('(?<!Baker: )\\b\\w+\\s\\w+$', flags = re.IGNORECASE|re.M)\n",
    "\n",
    "#Problem version: word boundary must be included to make sure there is a space to the left\n",
    "# (?<!Baker: ) says our match should not br preceded by Baker: and space. The first letter\n",
    "# after such a part IS preceded by that, so it will not be captured. With word boundary lookback\n",
    "# will check whether preceding is done; if yes, cursor comes back, and \\b makes sure that we will\n",
    "# force there to be a word boundary and thus also capture the first letter\n",
    "pattern3 = re.compile(r'(?<!Baker: )\\w+\\s\\w+$', flags = re.IGNORECASE|re.M)\n",
    "\n",
    "# Needs 'end-of' meta character $, othrewise does not work correctly\n",
    "# as we are not looking at END OF A LINE (notice that we have flag re.M) \n",
    "pattern4 = re.compile(r'(?<!Baker: )\\b\\w+\\s\\w+', flags = re.IGNORECASE|re.M)\n",
    "\n",
    "\n",
    "print(re.findall(pattern1, string))\n",
    "print('------')\n",
    "print(re.findall(pattern2, string))\n",
    "print(re.findall(pattern3, string))\n",
    "print(re.findall(pattern4, string))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookarounds do not match consecutively!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookarounds do not automatically work 'back-to-back' as the next examples demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''cherry 100 red \n",
    "            apple  150 green\n",
    "            grapes 200 \n",
    "            '''\n",
    "\n",
    "# Due to zero width assertion of lookarounds (in this case positive lookaheads),\n",
    "# we correctly match the middle column numbers with 1st lookahead but since the \n",
    "# cursor does not csonsume them (returns to start) the 2nd lookahead is not satisfied\n",
    "# as we do not find spces between 2nd and 3rd columns but rather the middle column number again! \n",
    "pattern1 = re.compile(r'[a-z]+\\s*(?= \\d+)(?=\\s*)(?=[a-z]+)')\n",
    "\n",
    "# Correcting patter1: putting all lookaheads into same group\n",
    "pattern2 = re.compile(r'[a-z]+\\s*(?= \\d+\\s*[a-z]+)')\n",
    "\n",
    "print(re.findall(pattern1, string))\n",
    "print(re.findall(pattern2, string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cook book examples for regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match if and only if (AND) certain conditions take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will match whole string if certain conditions take place.\n",
    "# We use lookarounds to achieve this \n",
    "string1 = 'AZN#3232!abbb32..'\n",
    "string2 = 'AZN#3232abbb3232'\n",
    "\n",
    "# Match whole string (\\S) if \n",
    "#  - uncapitalized letter is found after zero or more any character except newline\n",
    "#    AND\n",
    "#  - capitalized letter is found after zero or more any character except newline\n",
    "#    AND\n",
    "#  - number is found after zero or more any character except newline\n",
    "#    AND\n",
    "#  - special character (! OR ? OR .) is found after zero or more any character except newline\n",
    "pattern = re.compile('(?=.*[a-z])(?=.*[A-Z])(?=.*[0-9])(?=.*[!?.])\\S+')\n",
    "\n",
    "print(re.search(pattern, string1))\n",
    "print(re.search(pattern, string2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Despite the CONSTANT negative #press covfefe, @WH. Also, negative.\"\n",
    "straslist = str1.split()\n",
    "\n",
    "# Find occurence of a word from a sentence\n",
    "print([w for w in straslist if re.search('negative',w)])\n",
    "print(re.findall('negative',str1))\n",
    "print([i for i, w in enumerate(straslist) if re.search('negative',w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all English wovels/consonants from a word\n",
    "str1 = 'ouagadougou'\n",
    "print(re.findall(r'[aeiouy]', str1))\n",
    "print(re.findall(r'[^aeiouy]', str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find different dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = '''\n",
    "This is some sample text with different dates.\n",
    "02/25/2001; 02/25/01; 2/20/01; 2/3/01;\n",
    "2001-02-25; 2001/25/02; 2001-2-25; 2001-12-4;\n",
    "Feb-25-2001; Feb 25, 2001; February 25, 2001; Feb. 25, 2001; Feb 25 2009; July 3 2009;\n",
    "Feb-3-2001; Feb 3, 2001; February 3, 2001; Feb. 3, 2001; Feb 3 2009;\n",
    "25 Feb 2001; 25 February 2001; 25 Feb. 2001; 25 February, 2009;\n",
    "Feb 25th, 2009; March 21st, 2009; Aug 2nd, 2009; Aug 23rd, 2009;\n",
    "Feb 2001; September 2005; Oct 2010; 6/2001; 11/2001\n",
    "'''\n",
    "print(\"There are \" + str(len(str1.split(';'))) + \" dates\")\n",
    "\n",
    "# Find dates that are given in either of following formats \n",
    "#   -> 12-11-2002\n",
    "#   -> 12/11/2002\n",
    "#   -> 11/12/2002\n",
    "#   -> 12/11/02\n",
    "# word boundary at start included so that it won't falsely pick up\n",
    "# '2001-02-25' as '01-02-25'\n",
    "pattern1a = re.compile(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}')\n",
    "\n",
    "# Find dates that are given in either of following formats \n",
    "#   -> Nov-12-2002\n",
    "pattern1b = re.compile(r'[A-Za-z]{3}[/-]\\d{1,2}[/-]\\d{2,4}')\n",
    "\n",
    "# Find dates that are given in either of following formats \n",
    "#   -> 12 Nov 2002\n",
    "#   -> 12 November 2002\n",
    "#   -> Nov 12, 2002\n",
    "#   -> November 23, 2002\n",
    "#   -> Aug 2nd, 2009\n",
    "#   -> Feb. 3, 2001\n",
    "#   -> 25 Feb. 2001\n",
    "#   -> Feb 25 2009\n",
    "#   -> July 3 2009\n",
    "#   -> 25 February, 2009\n",
    "pattern2 = re.compile(r'(?:\\d{2} )?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z\\.]*,? (?:\\d{1,2}[a-z]{0,2},? )?\\d{4}') # 20 matches\n",
    "\n",
    "\n",
    "# Find dates that are given in either of following formats \n",
    "#   -> 6/2002 and 11/2002\n",
    "#   -> 6-2002 and 11-2002\n",
    "#pattern3 = re.compile(r'(?:[^\\w/-])(\\d{1,2}[/-]\\d{4})\\b') # does not match if date is at the start of the string!\n",
    "pattern3 = re.compile(r'(?<![\\w/-])(\\d{1,2}[/-]\\d{4})\\b') # should work now\n",
    "\n",
    "# Find dates that are given in either of following formats \n",
    "#   -> 2001-02-25\n",
    "#   -> 2001/02/25\n",
    "#   -> 2001-2-25\n",
    "pattern4 = re.compile(r'\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}')\n",
    "\n",
    "\n",
    "total_list = re.findall(pattern1a,str1) \\\n",
    "                + re.findall(pattern1b,str1) \\\n",
    "                + re.findall(pattern2,str1) \\\n",
    "                + re.findall(pattern3,str1) \\\n",
    "                + re.findall(pattern4,str1)\n",
    "\n",
    "#total_list = re.findall(pattern2,str1)\n",
    "\n",
    "\n",
    "print(\"We found \" + str(len(total_list)))\n",
    "print(total_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>re</i> module has three categories: pattern matching, substitution, and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = '0    Montevallo (University of Montevallo)[2]'\n",
    "\n",
    "# Find if contains brakcet\n",
    "pattern = re.compile('\\}')\n",
    "ff = pattern.findall(str1)\n",
    "gg = pattern.search(str1)\n",
    "\n",
    "if gg:\n",
    "    print('True')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get everything in the string before certain character (here '-')\n",
    "# If character not found, then get whole string\n",
    "str1 = 'a@b-c-d-e'\n",
    "str2 = 'a@bcde'\n",
    "\n",
    "regex1 = re.compile(r\"^([^-]*).*\")\n",
    "\n",
    "print(re.search(regex1, str1).group(1))\n",
    "print(re.search(regex1, str2).group(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get everything in the string after certain character (here '.')\n",
    "# If character not found, both approaches throw an error\n",
    "str1 = 'a@b-c-d-e'\n",
    "str2 = 'a@bcde'\n",
    "\n",
    "# First approach\n",
    "regex1 = re.compile(\"(?:-).*\")\n",
    "print(re.search(regex1, str1).group(0))\n",
    "#print(re.search(regex1, str2).group(0))\n",
    "\n",
    "# Second approach\n",
    "print(str1.split('-', 1)[1])\n",
    "#str2.split('-', 1)[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stuff between some tags (does not work with nested!)\n",
    "string  = 'This is a <stupid> sentence'\n",
    "match = re.compile('[\\<].*?[\\>]')\n",
    "string = match.sub('',string)\n",
    "print(re.sub( '\\s+', ' ', string).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove certain character\n",
    "string = 'Some string with \"number that are immportant: 22, 44, \"66'\n",
    "string.replace('\"', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove everything after last number\n",
    "\n",
    "regex1 = re.compile(r\"(.*?)(\\d)\")\n",
    "regex2 = re.compile(r\"^([^\\d]*)\\d*\")\n",
    "\n",
    "string = 'I need to 566 get everything 068 after digit'\n",
    "string2 = '1302 Pysäköinti 5 m matkalle ennen suojatietä'\n",
    "\n",
    "print(re.sub(regex1,'', string))\n",
    "\n",
    "print(re.sub(regex2,'', string))\n",
    "print(re.sub(regex2,'', string2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = '9999999 Unknown reason'\n",
    "str1 = list(map(str.strip, str1))\n",
    "str1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useless words as far as any data analysis goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "words = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sent = []\n",
    "filtered_sent = [w for w in words if not w in stop_words]\n",
    "        \n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a character sequence (some text) and a defined unit (sentence or word), <b>tokenization</b> is the task of chopping the sequence up into pieces (defined units), called <i>tokens</i> , perhaps at the same time throwing away certain characters, such as punctuation. This is why it is better in isolating words than e.g. re.spli(' ')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print('-'*15)\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# This lemmatizer takes in a part of speec parameter pos\n",
    "# If not supplied, defaults to \"noun\" which may not produce what we want\n",
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heading is pretty selg explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.\n",
      "------------------------------\n",
      "['President', 'George', 'W.', 'Bush', 'reacts', 'to', 'applause', 'during', 'his', 'State', 'of', 'the', 'Union', 'Address', 'at', 'the', 'Capitol', ',', 'Tuesday', ',', 'Jan', '.']\n",
      "------------------------------\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "# Train custom sentence tokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "# Tokenize test sample into sentences\n",
    "tokenized = custom_sent_tokenizer.tokenize(test_text)\n",
    "\n",
    "# Loop over some tokenized sentences\n",
    "tok_sent = tokenized[4:5][0]\n",
    "print(tok_sent)\n",
    "\n",
    "# Tokenize words\n",
    "words = nltk.word_tokenize(tok_sent)\n",
    "print('-'*30)\n",
    "print(words)\n",
    "\n",
    "# Tag parts of speech to words\n",
    "tagged = nltk.pos_tag(words)\n",
    "print('-'*30)\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing chnked trees is hard in Jupyter notebook environment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n",
      "------------------------------\n",
      "(S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  his/PRP$\n",
      "  (Chunk State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP)\n",
      "  ,/,\n",
      "  (Chunk Tuesday/NNP)\n",
      "  ,/,\n",
      "  (Chunk Jan/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Use variable \"tagged\" from above (Tagging part of speech)\n",
    "\n",
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "\n",
    "print(tagged)\n",
    "print('-'*30)\n",
    "print(chunked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to chunking but actually the opposite... fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can spot stuff like\n",
    "\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 'meaningful')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import words\n",
    "import pandas as pd\n",
    "\n",
    "correct_spellings = words.words()\n",
    "distances = []\n",
    "\n",
    "gram_n = 2\n",
    "entry = 'meanignful'\n",
    "\n",
    "'''\n",
    "Either correct_spellings or correct_spellings_redu can ne used.\n",
    "In latter choices are limited to words that start with same letter\n",
    "as the entry word\n",
    "'''\n",
    "correct_spellings_redu = pd.Series(correct_spellings)\n",
    "correct_spellings_redu = correct_spellings_redu[correct_spellings_redu.str.startswith(entry[0])]\n",
    "for word in correct_spellings:\n",
    "    myngrams1 = set(ngrams(word, gram_n))\n",
    "    myngrams2 = set(ngrams(entry, gram_n))\n",
    "    distances.append((jaccard_distance(myngrams1 , myngrams2), word))\n",
    "\n",
    "min_dist = min(distances)\n",
    "min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit distance (Damerau–Levenshtein distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'meaningful')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "import pandas as pd\n",
    "\n",
    "correct_spellings = words.words()\n",
    "distances = []\n",
    "\n",
    "entry = 'meanignful'\n",
    "\n",
    "'''\n",
    "Either correct_spellings or correct_spellings_redu can ne used.\n",
    "In latter choices are limited to words that start with same letter\n",
    "as the entry word\n",
    "'''\n",
    "correct_spellings_redu = pd.Series(correct_spellings)\n",
    "correct_spellings_redu = correct_spellings_redu[correct_spellings_redu.str.startswith(entry[0])]\n",
    "for word in correct_spellings:\n",
    "    distances.append((edit_distance(word , entry), word))\n",
    "\n",
    "min_dist = min(distances)\n",
    "min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "686px",
    "left": "0px",
    "right": "1240px",
    "top": "111px",
    "width": "296px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
